<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dataset Information</title>
    <link rel="stylesheet" href="../CSS/header.css">
    <link rel="stylesheet" href="../CSS/about.css">
    <script src="../js/includeHTML.js" defer></script>
    <link rel="icon" type="image/x-icon" href="..\DiaCorpus_Mafat\images\favicons\favicon.ico">
    <style>
        /* Custom paragraph with adjusted margins */
        custom-p {
            margin-top: -10px;
            font-size: 1.2em;
            line-height: 1.6;
            text-align: justify;
            color: #333;
        }

        /* Increase bullet point size */
        .info-list {
            list-style-type: disc;
            list-style-position: inside;
            font-size: 1.1em; /* Increase the size of the bullet point */
        }

        /* Adjust line spacing between list items */
        .info-list li {
            line-height: 1.2; /* Decrease line height to shorten spacing between lines */
            margin-bottom: 5px; /* Optional: add space between each list item for readability */
        }

        /* Indent additional lines */
        .info-list li {
            text-indent: -1.4em; /* Adjust this value according to your bullet size */
            padding-left: 1.1em; /* Ensure the first line aligns with the bullet */
        }
    </style>
</head>
<body>
    <!-- Include the header component -->
    <div data-include="header.html"></div>

    <div class="project-container">
        <h1>Dataset Information</h1>

        
        <custom-p>
            Tagging texts is largely subjective, as personal experiences, pre-existing perceptions, thoughts, and beliefs of the annotator influence it. Therefore, it is essential for the tagging instructions to be clear to ensure the annotators remain objective.
        </custom-p>

        <h2>Sentiment Analysis Dataset</h2>
        
        <h3>Evaluation Reliability</h3>
        <custom-p>
            To mitigate subjectivity, we ensured that there were at least three different annotators for each sentiment tag. In cases where there was disagreement among all three annotators, additional reviewers were assigned. Annotators were also instructed to assess sentiment from the writer's point of view.
        </custom-p>
        
        <h3>General Information:</h3>
        <custom-p>The database includes 30,000 sentences selected according to the following criteria:</custom-p>
        <ul class="info-list">
            <li>
                Sentences are written spontaneously by humans. The sentences do not contain additional types of information 
                <br>
                (such as HTML, tables, images, etc.).
            </li>
            <li>The content of the sentences covers a variety of topics.</li>
            <li>The source of the information includes a combination of sentences and paragraphs.</li>
            <li>
                The sentences were written in the local dialect of Arabic, with most of the sentences written in the colloquial language commonly used 
                <br>
                on social networks.
            </li>
        </ul>

        <h3>Sources:</h3>
        <custom-p>The sentences were taken from two main sources:</custom-p>
        <ul class="info-list">
            <li>Tweets from Twitter: 20,000 tweets</li>
            <li>YouTube video comments: 10,000 comments</li>
        </ul>

        <h3>Corpus Domain Distribution:</h3>
        <custom-p>The corpus contains sentences from various domains:</custom-p>
        <ul class="info-list">
            <li>General content: 34%</li>
            <li>Sports: 12.8%</li>
            <li>Religious content: 28%</li>
            <li>Content with Religious Elements: 5.6%</li>
            <li>Political content: 10.4%</li>
            <li>Promotional content: 1.6%</li>
            <li>Criminal-Security content: 0.8%</li>
            <li>Content that cannot be categorized: 6.8%</li>
        </ul>

        <h3>Diversity of Topics:</h3>
        <custom-p>The texts were categorized into the following sentiment tags:</custom-p>
        <ul class="info-list">
            <li><strong>Positive:</strong> Expressing a positive or optimistic sentiment.</li>
            <li><strong>Negative:</strong> Expressing a critical, non-positive, or pessimistic sentiment.</li>
            <li><strong>Neutral:</strong> Sentences without strong positive or negative sentiment.</li>
            <li><strong>Complex:</strong> When the sentiment is unclear, ambiguous, or context-dependent.</li>
        </ul>
        
        <h2>Coreference Resolution</h2>
        <h3>Evaluation Reliability</h3>

        <custom-p>
            To reduce subjectivity, every file was reviewed and refined by an experienced annotator after the initial tagging.
        </custom-p>
        
        <h3>General Information:</h3>
        <custom-p>The dataset consists of 73 annotated transcriptions of podcasts and interviews selected based on the following criteria:</custom-p>
        <ul class="info-list">
            <li>All texts are in spoken Arabic, specifically in the Palestinian dialect.</li>
            <li>The texts were transcribed using automated tools and refined by human annotators for accuracy.</li>
            <li>The information sources include complex dialogues, Q&A sessions, and intricate references.</li>
        </ul>
        
        <h3>Sources of Data</h3>
        <custom-p>The texts were taken from two main sources:</custom-p>
        <ul class="info-list">
            <li><strong>32 YouTube podcasts</strong> by Ahmad Bikawi, transcribed and refined in spoken Arabic (Palestinian dialect). Each podcast is approximately 1 hour and 20 minutes long, totaling around 42 hours of speech.</li>
            <li><strong>41 Original interviews</strong>, recorded, transcribed, and refined between native Arabic speakers in the Palestinian dialect. Each interview lasts around 21 minutes on average, with a total of approximately 14 hours of speech.</li>
        </ul>
        
        <h3>Quality Control</h3>
        <custom-p>After completing the initial annotation, all files were reviewed and refined by two experienced annotators. Following the QA process, the files were re-evaluated, resulting in the following statistics:</custom-p>
        <ul class="info-list">
            <li><strong>Number of files in dataset:</strong> 73</li>
            <li><strong>Total coreference tags:</strong> 76,968</li>
            <li><strong>Number of words:</strong> 535,248</li>
            <li><strong>Average coreferences per file:</strong> 1,054</li>
            <li><strong>Average coreferences per annotator:</strong> 12,828</li>
        </ul>
        
        <h3>Metrics</h3>
        <custom-p>To assess the refinement process, a comparison was made between the first annotator version and the second version after QA. The following metrics were calculated:</custom-p>
        <ul class="info-list">
            <li><strong>MUC (Mention Understanding Conference):</strong> Evaluates the structural similarity of coreference chains: <strong>0.89</strong></li>
            <li><strong>B³ (Bagga and Baldwin):</strong> Assesses coreference resolution by considering each mention individually - <strong>F1 Score: 0.71</strong></li>
            <li><strong>Cohen's Kappa:</strong> Measures inter-annotator agreement: <strong>0.81</strong></li>
            <li><strong>Weighted Coreference Agreement:</strong> This metric averages the accuracy of each coreference match, weighted by exact matches (1.0), partial span matches (0.75), partial label matches (0.5), and misses (0.0), to reflect the overall agreement between the predicted and ground truth annotations: <strong>0.66</strong></li>
        </ul>

        <custom-p><em>* This dataset is provided post-refinement.</em></custom-p>


        <h2>Summarization</h2>
        <h3>Evaluation Reliability</h3>

        <custom-p>
            To ensure objectivity, a selected batch of summaries was reviewed and refined by an experienced annotator after the initial summarization.
        </custom-p>

        <h3>General Information:</h3>
        <custom-p>
            The dataset consists of 4,690 news articles and 44 transcribed interviews, selected according to the following criteria:
        </custom-p>
        <ul class="info-list">
            <li>All news articles are written in Modern Standard Arabic (MSA), the language typically used for journalism.</li>
            <li>The news articles focus on the Israeli-Palestinian region to capture the influence of local dialects.</li>
            <li>The interview texts are in spoken Arabic, specifically in the Palestinian dialect.</li>
            <li>The interviews were transcribed using automated tools and then refined by human annotators for accuracy. They include dialogues and Q&A formats.</li>
        </ul>
    
        <h3>Sources of the Texts</h3>
        <ul class="info-list">
            <li>
                <strong>4,690 news articles</strong> of varying lengths (35 – 600 words) were provided courtesy of Yifat Media Information. These articles, from recent years, were sourced from both print and online media covering the Israeli-Palestinian context.
            </li>
            <li>
                <strong>44 Original interviews</strong>, recorded, transcribed, and refined between native Arabic speakers in the Palestinian dialect. Each interview lasts around 21 minutes on average, with a total of approximately 15 hours of speech.
            </li>
        </ul>
    
        <h3>Quality Control</h3>
        <ul class="info-list">
        <li>
            <strong>News Articles:</strong> After summarization, a sample of the files was selected for additional review and refinement by the original summarizers. Each reviewer assessed a summary produced by a different summarizer, ensuring that no one reviewed their own work and that every reviewer was assigned a new, unfamiliar summary. A total of 304 news article summaries underwent QA process, 300 of them required adjustments.
        <br>
        <br>
        <strong>Post-QA Metrics</strong>
        <br>
        
        <ul class="info-list" style = "font-size :16px;">
            <li><strong>ROUGE-1 F1 Score:</strong> 0.815</li>
            <li><strong>ROUGE-2 F1 Score:</strong> 0.7656</li>
            <li><strong>ROUGE-L F1 Score:</strong> 0.795</li>
            <li><strong>BERTScore:</strong> 0.91</li>
        </ul>
        <br>
        
            BERTScore measures the semantic similarity between two texts using BERT-based embeddings, while ROUGE also measures semantic similarity but is more sensitive to word overlap. A BERTScore of 0.91 indicates that evaluators agreed on the main ideas and overall meaning of the summaries. The ROUGE scores suggest that while there is some overlap in exact wording, the summaries differ in surface-level structure, phrasing, and vocabulary.
        </li>
    </ul>

        <ul class="info-list">
            <li>
            <strong>Interviews:</strong> After the initial summarization, all interview files were reviewed and refined by two experienced summarizers. The summaries were then incorporated into the dataset following QA.
            </li>
        </ul>
        <custom-p><em>* This dataset is provided post-refinement.</em></custom-p>
    </div>
</body>
</html>
